<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="./favicon.ico">
    <link rel="icon" type="image/png" href="./favicon.png">
    <link rel="stylesheet" href="./index.css">
    <title>Elmir Mohammed - Pig CV Project</title>
</head>
<body>
    <nav>
        <div id="nav-container">
            <div id="nav-content">
                <a href="./index.html" class="name">Elmir</a>
                <div id="right-links">
                    <a href="./index.html">Home</a>
                    <a href="./about.html">About</a>
                    <a href="./research.html">Research</a>
                    <a href="./works.html">Works</a>
                    <a href="./awards.html">Awards</a>
                </div>
            </div>
        </div>
    </nav>
    <section id="profile">
        <div class="profile-container">
            <div class="profile-left">
                <div id="img">
                    <div id="img-cont">
                        <img src="./img/el.jfif" alt="">
                    </div>
                </div>
                <div class="profile-info">
                    <h2>Elmir Mohammed</h2>
                    <p class="profile-bio">
                        Electrical and Electronics Engineering graduate from Ashesi University, specializing in Embedded Machine Learning, IoT, and FPGA-based architectures for edge computing.
                    </p>
                    <div class="contact-links">
                        <div class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M8 16s6-5.686 6-10A6 6 0 0 0 2 6c0 4.314 6 10 6 10zm0-7a3 3 0 1 1 0-6 3 3 0 0 1 0 6z"/>
                            </svg>
                            <span>Ghana</span>
                        </div>
                        <a href="mailto:mohammed.elmir@alumni.ashesi.edu.gh" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
                            </svg>
                            <span>Email</span>
                        </a>
                        <a href="https://www.linkedin.com/in/elmir-mohammed-82b9201b7/" target="_blank" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
                            </svg>
                            <span>LinkedIn</span>
                        </a>
                        <a href="https://github.com/Knight-Khode" target="_blank" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
                            </svg>
                            <span>GitHub</span>
                        </a>
                    </div>
                </div>
            </div>
            <div class="profile-right">
                <div style="display: flex; justify-content: center; margin-bottom: 60px;">
                    <video width="900" height="360" controls autoplay muted loop>
                        <source src="./video/Media1.mp4" type="video/mp4">
                    </video>
                </div>

        <div id="content-cv">
            <div style="width: 90vw; margin: auto;">
                <p style="font-size: 25px; text-align: center; margin-bottom: 60px;">
                    <span style="font-weight: bold;">Monitoring of Animal Movement using Computer Vision</span>
                </p>
                <h2>Overview:</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">This project presents a non-intrusive, real-time computer vision system for monitoring pigs in farm environments. The system identifies individual pigs by breed and tracks their movements to assess health and behavior patterns. By leveraging deep learning models (YOLOv8n, MobileNetSSD, EfficientNet-B0) and deploying on a Raspberry Pi 4, the system achieved up 
                    to 97% accuracy, making it practical for precision livestock farming.</p>
                    <br>
                    <br>
                <h2>Problem Statement</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">Traditional methods of monitoring pig behavior in Africa and beyond rely heavily on manual 
                    observation or sensor-based approaches (accelerometers, RFID tags).</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Manual observation is labor-intensive, prone to human error, and inefficient for large farms.
                    </li>
                    <li>
                        Sensor-based approaches are intrusive, cause pain/biosecurity risks for animals, and can malfunction when pigs roll or fight.
                    </li>
                </ul>
                <b>A low-cost, automated, and non-intrusive alternative was needed.</b>
                <br>
                <br>
                <h2>Proposed Solution</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">We designed a computer vision system that:</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Identifies individual pig breeds (Berkshire, Duroc, Landrace, Pietrain).
                    </li>
                    <li>
                        Tracks movement across frames using a centroid-based algorithm with Euclidean distance.
                    </li>
                    <li>
                        Deployed on a Raspberry Pi 4B with a camera module for farm-ready integration.
                    </li>
                    <li>
                        Provides farmers with time-series visualizations of movement patterns, enabling health monitoring, stress detection, and feeding habit analysis.
                    </li>
                </ul>
                <br>
                <h2>Personal Contributions</h2>
                <ul style="padding-left: 40px;">
                    <li>
                        Co-authored a paper published on IEEE Xplore <a href="https://ieeexplore.ieee.org/document/10856474" style="display: inline-block;" target="_blank">Link</a>
                    </li>
                    <li>
                        Co-led system design, data collection, and preprocessing.
                    </li>
                    <li>
                        Implemented model training (YOLOv8n, MobileNetSSD, EfficientNet-B0).
                    </li>
                    <li>
                        Deployed trained models on Raspberry Pi 4.
                    </li>
                    <li>
                        Developed centroid-based movement tracking with OpenCV.
                    </li>
                </ul>
                <br>
                <h2>Technical Approach</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">1. Data Collection</p>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <img src="./img/data_collection.png" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        4 pig breeds, same weight (~25kg) and age (~8 weeks).
                    </li>
                    <li>
                        Collected 2 weeks of video data using a V380 Pro Wi-Fi PTZ CCTV camera and Raspberry Pi Camera Module 3.
                    </li>
                    <li>
                        Extracted 10,676 images for training.
                    </li>
                </ul>
                <br>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <div>
                        <img src="./img/pi_data.jpg" width="400px" alt="">
                    </div>
                    <div>
                        <img src="./img/cctv_data.jpg" width="400px">
                    </div>
                </div>
                <div style="display: flex; justify-content: center;">
                    <small style="text-align: center;">On-Field 
                    Data collection with Pi-camera and CCTV camera</small>
                </div>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">2. Pre-processing</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Annotation with RoboFlow.
                    </li>
                    <li>
                        Applied 7 augmentation techniques (flip, rotation, grayscale, brightness, blur, noise, crop).
                    </li>
                    <li>
                        Resized images to 320×320.
                    </li>
                    <li>
                        Data split: 82% training, 12% testing, 6% validation.
                    </li>
                </ul>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">3. Model Training</p>
                <div style="display: flex; justify-content: center;">
                    <img src="./img/performance.png" width="700px" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        Compared YOLOv8n, MobileNetSSD, EfficientNet-B0.
                    </li>
                    <li>
                        Training on Google Colab using transfer learning and fine-tunning
                    </li>
                    <li>
                        Incorporated pytorch, tensorFlow and tensorFlow-lite
                    </li>
                    <li>
                        Results:
                        <ul style="padding-left: 30px;">
                            <li>YOLOv8n → <b>97% accuracy</b></li>
                            <li>MobileNetSSD → <b>94.6% accuracy</b></li>
                            <li>EfficientNet-B0 → <b>72% accuracy</b></li>
                        </ul>
                    </li>
                </ul>
                <div style="display: flex; justify-content: center;">
                    <small style="text-align: center;">YOLOv8n performed the best thus selected for deployment on MCU</small>
                </div>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">4. Implemented Approach (Centroid Tracking)</p>
                <div style="display: flex; justify-content: center;">
                    <img src="./img/centroidal.png" width="600px" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        After detecting pigs with <b>YOLOv8n</b>, each pig is localized with a bounding box. 
    A centroid point <i>(C<sub>x</sub>, C<sub>y</sub>)</i> is calculated as:
                    </li>
                    <p style="text-align:center; font-size:18px;">
    $$C_x = x_1 + \frac{w}{2}, \quad C_y = y_1 + \frac{h}{2}$$
  </p>
                    <li>
                        Movement is then determined by comparing centroids across consecutive frames 
    using the Euclidean distance:
                    </li>
                     <p style="text-align:center; font-size:18px;">
    $$D = \sqrt{(C_{x1} - C_{x2})^2 + (C_{y1} - C_{y2})^2}$$
  </p>            <li>
                        
                        If <i>D</i> exceeds a threshold of <b>10 pixels</b>, 
    the pig is considered <b>moving</b>. 
    Otherwise, it is considered <b>stationary</b>.
                    </li>
                </ul>

                <p>This work has been extended to monitor individual animal feeding habits <a href="./feeding_cv.html" target="_blank">Read Here</a></p>
                </div>
            </div>
        </div>
    </section>
    
    <footer>
        <div class="footer-content">
            <p>&copy; 2025 Elmir Mohammed</p>
        </div>
    </footer>
</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>