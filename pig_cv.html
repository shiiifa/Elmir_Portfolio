<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="./favicon.ico">
    <link rel="icon" type="image/png" href="./favicon.png">
    <link rel="stylesheet" href="./index.css">
    <style>
        #profile .profile-container {
            margin: 120px auto 0 auto !important;
            display: block !important;
            max-width: 1200px;
            width: 100%;
        }
        #profile .profile-right {
            max-width: 100% !important;
            padding-left: 0 !important;
        }
    </style>
    <title>Elmir Mohammed - Pig CV Project</title>
</head>
<body>
    <a href="./works.html" class="back-button">← Back</a>
    <nav>
        <div id="nav-container">
            <div id="nav-content">
                <a href="./index.html" class="name">Elmir</a>
                <div id="right-links">
                    <a href="./index.html">Home</a>
                    <a href="./research.html">Academics and Research</a>
                    <a href="./works.html">Works</a>
                    <a href="./awards.html">Awards</a>
                    <a href="./Resume_Elmir.pdf">CV</a>
                </div>
            </div>
        </div>
    </nav>
    <section id="profile">
        <div class="profile-container">
            <div class="profile-right">
                <h1 style="color: #640807; text-align: center; font-size: 32px; margin-bottom: 40px; font-weight: bold;">Monitoring of Animal Movement using Computer Vision</h1>
                <div style="display: flex; justify-content: center; align-items: center; margin-bottom: 60px;">
                    <video width="900" height="360" controls autoplay muted loop style="display: block; margin: 0 auto;">
                        <source src="./video/Media1.mp4" type="video/mp4">
                    </video>
                </div>

        <div id="content-cv">
            <div style="width: 80%; max-width: 800px; margin: auto;">
                <h2>Overview:</h2>
                <p style="text-align: center; font-size: 18px; line-height: 28px;">This project presents a non-intrusive, real-time computer vision system for monitoring pigs in farm environments. The system identifies individual pigs by breed and tracks their movements to assess health and behavior patterns. By leveraging deep learning models (YOLOv8n, MobileNetSSD, EfficientNet-B0) and deploying on a Raspberry Pi 4, the system achieved up 
                    to 97% accuracy, making it practical for precision livestock farming.</p>
                    <br>
                    <br>
                <h2>Problem Statement</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">Traditional methods of monitoring pig behavior in Africa and beyond rely heavily on manual 
                    observation or sensor-based approaches (accelerometers, RFID tags).</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Manual observation is labor-intensive, prone to human error, and inefficient for large farms.
                    </li>
                    <li>
                        Sensor-based approaches are intrusive, cause pain/biosecurity risks for animals, and can malfunction when pigs roll or fight.
                    </li>
                </ul>
                <b>A low-cost, automated, and non-intrusive alternative was needed.</b>
                <br>
                <br>
                <h2>Proposed Solution</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">We designed a computer vision system that:</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Identifies individual pig breeds (Berkshire, Duroc, Landrace, Pietrain).
                    </li>
                    <li>
                        Tracks movement across frames using a centroid-based algorithm with Euclidean distance.
                    </li>
                    <li>
                        Deployed on a Raspberry Pi 4B with a camera module for farm-ready integration.
                    </li>
                    <li>
                        Provides farmers with time-series visualizations of movement patterns, enabling health monitoring, stress detection, and feeding habit analysis.
                    </li>
                </ul>
                <br>
                <h2>Personal Contributions</h2>
                <ul style="padding-left: 40px;">
                    <li>
                        Co-authored a paper published on IEEE Xplore <a href="https://ieeexplore.ieee.org/document/10856474" style="display: inline-block;">Link</a>
                    </li>
                    <li>
                        Co-led system design, data collection, and preprocessing.
                    </li>
                    <li>
                        Implemented model training (YOLOv8n, MobileNetSSD, EfficientNet-B0).
                    </li>
                    <li>
                        Deployed trained models on Raspberry Pi 4.
                    </li>
                    <li>
                        Developed centroid-based movement tracking with OpenCV.
                    </li>
                </ul>
                <br>
                <h2>Technical Approach</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">1. Data Collection</p>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <img src="./img/data_collection.png" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        4 pig breeds, same weight (~25kg) and age (~8 weeks).
                    </li>
                    <li>
                        Collected 2 weeks of video data using a V380 Pro Wi-Fi PTZ CCTV camera and Raspberry Pi Camera Module 3.
                    </li>
                    <li>
                        Extracted 10,676 images for training.
                    </li>
                </ul>
                <br>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <div>
                        <img src="./img/pi_data.jpg" width="400px" alt="">
                    </div>
                    <div>
                        <img src="./img/cctv_data.jpg" width="400px">
                    </div>
                </div>
                <div style="display: flex; justify-content: center;">
                    <small style="text-align: center;">On-Field 
                    Data collection with Pi-camera and CCTV camera</small>
                </div>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">2. Pre-processing</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Annotation with RoboFlow.
                    </li>
                    <li>
                        Applied 7 augmentation techniques (flip, rotation, grayscale, brightness, blur, noise, crop).
                    </li>
                    <li>
                        Resized images to 320×320.
                    </li>
                    <li>
                        Data split: 82% training, 12% testing, 6% validation.
                    </li>
                </ul>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">3. Model Training</p>
                <div style="display: flex; justify-content: center;">
                    <img src="./img/performance.png" width="700px" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        Compared YOLOv8n, MobileNetSSD, EfficientNet-B0.
                    </li>
                    <li>
                        Training on Google Colab using transfer learning and fine-tunning
                    </li>
                    <li>
                        Incorporated pytorch, tensorFlow and tensorFlow-lite
                    </li>
                    <li>
                        Results:
                        <ul style="padding-left: 30px;">
                            <li>YOLOv8n → <b>97% accuracy</b></li>
                            <li>MobileNetSSD → <b>94.6% accuracy</b></li>
                            <li>EfficientNet-B0 → <b>72% accuracy</b></li>
                        </ul>
                    </li>
                </ul>
                <div style="display: flex; justify-content: center;">
                    <small style="text-align: center;">YOLOv8n performed the best thus selected for deployment on MCU</small>
                </div>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">4. Implemented Approach (Centroid Tracking)</p>
                <div style="display: flex; justify-content: center;">
                    <img src="./img/centroidal.png" width="600px" alt="">
                </div>
                <ul style="padding-left: 40px;">
                    <li>
                        After detecting pigs with <b>YOLOv8n</b>, each pig is localized with a bounding box. 
    A centroid point <i>(C<sub>x</sub>, C<sub>y</sub>)</i> is calculated as:
                    </li>
                    <p style="text-align:center; font-size:18px;">
    $$C_x = x_1 + \frac{w}{2}, \quad C_y = y_1 + \frac{h}{2}$$
  </p>
                    <li>
                        Movement is then determined by comparing centroids across consecutive frames 
    using the Euclidean distance:
                    </li>
                     <p style="text-align:center; font-size:18px;">
    $$D = \sqrt{(C_{x1} - C_{x2})^2 + (C_{y1} - C_{y2})^2}$$
  </p>            <li>
                        
                        If <i>D</i> exceeds a threshold of <b>10 pixels</b>, 
    the pig is considered <b>moving</b>. 
    Otherwise, it is considered <b>stationary</b>.
                    </li>
                </ul>

                <p>This work has been extended to monitor individual animal feeding habits <a href="./feeding_cv.html">Read Here</a></p>
                </div>
            </div>
        </div>
    </section>
    
    <footer>
        <div class="footer-content">
            <p>&copy; 2025 Elmir Mohammed</p>
        </div>
    </footer>
</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>