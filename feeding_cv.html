<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="./favicon.ico">
    <link rel="icon" type="image/png" href="./favicon.png">
    <link rel="stylesheet" href="./index.css">
    <title>Elmir Mohammed - Feeding CV Project</title>
</head>
<body>
    <nav>
        <div id="nav-container">
            <div id="nav-content">
                <a href="./index.html" class="name">Elmir</a>
                <div id="right-links">
                    <a href="./index.html">Home</a>
                    <a href="./about.html">About</a>
                    <a href="./research.html">Research</a>
                    <a href="./works.html">Works</a>
                    <a href="./awards.html">Awards</a>
                </div>
            </div>
        </div>
    </nav>
    <section id="profile">
        <div class="profile-container">
            <div class="profile-left">
                <div id="img">
                    <div id="img-cont">
                        <img src="./img/el.jfif" alt="">
                    </div>
                </div>
                <div class="profile-info">
                    <h2>Elmir Mohammed</h2>
                    <p class="profile-bio">
                        Electrical and Electronics Engineering graduate from Ashesi University, specializing in Embedded Machine Learning, IoT, and FPGA-based architectures for edge computing.
                    </p>
                    <div class="contact-links">
                        <div class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M8 16s6-5.686 6-10A6 6 0 0 0 2 6c0 4.314 6 10 6 10zm0-7a3 3 0 1 1 0-6 3 3 0 0 1 0 6z"/>
                            </svg>
                            <span>Ghana</span>
                        </div>
                        <a href="mailto:mohammed.elmir@alumni.ashesi.edu.gh" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
                            </svg>
                            <span>Email</span>
                        </a>
                        <a href="https://www.linkedin.com/in/elmir-mohammed-82b9201b7/" target="_blank" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
                            </svg>
                            <span>LinkedIn</span>
                        </a>
                        <a href="https://github.com/Knight-Khode" target="_blank" class="contact-item">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
                            </svg>
                            <span>GitHub</span>
                        </a>
                    </div>
                </div>
            </div>
            <div class="profile-right">
                <div style="display: flex; justify-content: center; margin-bottom: 60px;">
                    <video width="900" height="360" controls autoplay muted loop>
                        <source src="./video/Media2.mp4" type="video/mp4">
                    </video>
                </div>
    <p>This is an update</p>
        <div id="content-cv">
            <div style="width: 90vw; margin: auto;">
                <p style="font-size: 25px; text-align: center; margin-bottom: 60px;">
                    <span style="font-weight: bold;">Research Title:</span>  
                    Real Time Monitoring of Animal Feeding Behaviour Using Computer Vision & Embedded Systems
                </p>
                <h2>Overview:</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">
                    This project integrates computer vision (YOLOv8n), embedded systems (Raspberry Pi 4, ESP32), and IoT (MQTT + Firebase) to monitor individual pig feeding behaviour in real time. The system identifies pigs by breed or visual marking, detects entry to the feeding area, measures feeding duration, weighs feed before/after visits (using load cells), and logs per-visit feed consumption and timestamps to a cloud database for time-series analysis.
                </p>
                    <br>
                    <br>
                <h2>Problem Statement</h2>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">
                    Manual monitoring is labour-intensive and sensor tags are intrusive (biosecurity and welfare issues). Key challenges: reliably identifying individuals (especially same-breed pigs), measuring per-visit feed intake, and building a real-time, farm-robust pipeline that runs on low-cost edge hardware.
                </p>
                <ul style="padding-left: 40px;">
                    <li>
                        End-to-end system combining vision, ultrasonic triggers, weight sensing (load cells), and MQTT communication.
                    </li>
                    <li>
                        Evaluated three identification/tracking strategies: marking (numbers), centroid tracking, and SORT (Kalman filter + Hungarian matching).
                    </li>
                    <li>
                        Demonstrated high detection performance using YOLOv8n (per-breed accuracies: Landrace 99%, Pietrain 95%, Duroc 97%, Berkshire 96%; mAP ≈ 97%).
                    </li>
                    <li>
                        Practical engineering to improve edge performance (image resize, 1-second sampling) and measured the accuracy/FPS tradeoff.
                    </li>
                </ul>
                <br>
                <h2>Personal Contributions</h2>
                <ul style="padding-left: 40px;">
                    <li>
                        Led the design and implementation of the system.
                    </li>
                    <li>
                        Developed the computer vision pipeline (YOLOv8n + OpenCV) for pig detection and feeding behavior tracking
                    </li>
                    <li>
                        Integrated ultrasonic sensors and load cells with ESP32 for feed measurement.
                    </li>
                    <li>
                        Set up IoT communication (MQTT) between Raspberry Pi, ESP32, and cloud server.
                    </li>
                    <li>
                        Drafted the research paper, which is under review for IEEE publication (Lead Author).
                    </li>
                </ul>
                <br>
                <br>
                <h2>System architecture (three subsystems)</h2>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <img src="./img/system_architecture.jpg" width="500px" alt="">
                </div>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">1. Feed weight subsystem</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Four 20-kg load cells under feed troughs.
                    </li>
                    <li>
                        ESP32 reads HX711 / load cell ADC.
                    </li>
                    <li>                   
                        Triggered by a pig entry event → capture pre-feed weight (FPA) and post-feed weight (FL).
                    </li>
                    <li>
                        Feed consumed per visit = FPA − FL.
                    </li>
                </ul>
                <br>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <div>
                        <img src="./img/schematic.jpg" width="500px" alt="">
                    </div>
                    <div>
                        <img src="./img/pcb.jpg" width="400px" alt="">
                    </div>
                </div>
                <div style="display: flex; justify-content: center;">
                    <small style="text-align: center;">Circuit Design & PCB</small>
                </div>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">2. Computer Vision System</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Camera: Raspberry Pi Camera Module 3 (12 MP, IMX708, 120° FOV).
                    </li>
                    <li>
                        Detection model: YOLOv8n (trained/fine-tuned on dataset).
                    </li>
                    <li>
                        Trigger: ultrasonic sensor detects presence within 60 cm → Pi starts 1-second interval captures and inference.
                    </li>
                    <li>
                        Tracking strategies: marking detection, centroid tracking, SORT algorithm.
                    </li>
                    <li>
                        Measures feeding duration by detecting centroid crossing virtual entry line and timing until leaving.
                    </li>
                </ul>
                <br>
                <p style="text-align: justify; font-size: 18px; line-height: 28px;">3. End-user subsystem</p>
                <ul style="padding-left: 40px;">
                    <li>
                        Raspberry Pi aggregates breed/ID, feeding duration, and weight delta.
                    </li>
                    <li>
                        Data published via MQTT from ESP32 → Pi → cloud; stored in Firebase.
                    </li>
                    <li>
                        Visualized as time-series (grams vs. time) for each pig.
                    </li>
            
                </ul>
                
                <br>
                <h2>Data & model details</h2>
                <ul style="padding-left: 40px;">
                    <li>Data captured: Two scenarios</li>
                    <ul style="padding-left: 40px;">
                        <li>
                            Different breeds: 10,676 images (train/test/val split as in paper).
                        </li>
                        <li>
                            Same-breed (three Landrace with numbered markings): 3,386 images.
                        </li>

                    </ul>
                    <li><b>YOLOv8n training: </b>350 epochs on Google Colab (Ultralytics).</li>
                    <li><b>Per-breed validation accuracies:</b>Landrace 99%, Pietrain 95%, Duroc 97%, Berkshire 96% (mAP ≈ 97%)</li>
                    <li><b>Image size vs FPS on Raspberry Pi: </b></li>
                    <ul style="padding-left: 40px;">
                        <li>640×640 → Accuracy 0.97, FPS ≈ 0.8</li>
                        <li>320×320 → Accuracy 0.96, FPS ≈ 2.1</li>
                    </ul>
                     <div style="display: flex; justify-content: center;">
                    <img src="./img/fps.png" width="200px" alt="">
                </div>
                <br>
                    <li><b>Marking model (Roboflow)</b> for same-breed numbering: mAP ≈ 84.6 (overall).</li>
                </ul>
                <br>
               <div style="display: flex; justify-content: center;">
                    <img src="./img/fps_rasp.gif" width="400px" alt="">
                </div>
                <div style="display: flex; justify-content: center;">
                    <small>Sample Detection on Raspberry Pi 4</small>
                </div>
                
                <h2>Assigning Unique ID to individual Pigs (Tracking approaches)</h2>
                <ul style="padding-left: 40px;">
                    <li><b>Marking (numbers):</b> high identification accuracy but intrusive & not scalable (markings fade).</li>
                    <div style="display: flex; justify-content: center;">
                    <iframe width="550" height="300" 
        src="https://www.youtube.com/embed/9gRNzxMdybg?autoplay=1&mute=1&loop=1&playlist=9gRNzxMdybg" 
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
</iframe>

                    </div>
                    <br>
                <li><b>Centroid positional change:</b> non-intrusive, lightweight; prone to ID swap when pigs cluster.</li>
                <div style="display: flex; justify-content: center;">
                    <iframe width="550" height="300" 
        src="https://www.youtube.com/embed/xciat4pTyWA?autoplay=1&mute=1&loop=1&playlist=xciat4pTyWA" 
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
</iframe>

                    </div>
                    <br>
                <li><b>SORT algorithm:</b> Good identity persistence under occlusion/clustering; heavier computationally (Kalman + data association).</li>  
                    
                    <div style="display: flex; justify-content: center;">
                    <iframe width="550" height="300" 
        src="https://www.youtube.com/embed/PutJVE_0e8Q?autoplay=1&mute=1&loop=1&playlist=PutJVE_0e8Q" 
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
</iframe>

                    </div>
                <br>
                <br>
               <h2 style="padding-bottom: 30px;">Tracking Feeding Behavior</h2> 
               
               <p style="text-align: justify; font-size: 18px; line-height: 28px;">
                To monitor feeding behavior, the system combines computer vision and sensors. An ultrasonic sensor is placed near the feeding area and detects the presence of an object within a 60 cm range. Once triggered, the sensor activates the computer vision subsystem on the Raspberry Pi to confirm whether the object is a pig and identify its breed.
                <br>
                Using OpenCV, virtual lines are drawn across the feeding region. When the centroid of a pig’s bounding box crosses this threshold, the system records the start of feeding. When the pig exits and the centroid crosses back out, the system records the end of feeding. The time difference gives the feeding duration.

At the same time, the feed weighing subsystem (with load cells connected to an ESP32) measures feed levels. Feed weight is captured when the pig enters (FPA) and again when it leaves (FL). The difference provides the feed consumed.
               </p>
               <div style="display: flex; justify-content: center; align-items: center;">
                    <div>
                        <img src="./img/feeding_arch.png" width="500px" alt="">
                    </div>
                    <div>
                        <img src="./img/feeding_pig.png" width="600px" alt="">
                    </div>
                </div>
                <div style="display: flex; justify-content: center;">
                    <iframe width="700" height="450" 
        src="https://www.youtube.com/embed/x0HBSc9V8H8?autoplay=1&mute=1&loop=1&playlist=x0HBSc9V8H8" 
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
</iframe>

                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <footer>
        <div class="footer-content">
            <p>&copy; 2025 Elmir Mohammed</p>
        </div>
    </footer>
</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>

